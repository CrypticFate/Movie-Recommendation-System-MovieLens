{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_wFg-18EnFR"
      },
      "source": [
        "# Movie Recommendation System – MovieLens\n",
        "\n",
        "### Task Overview:\n",
        "This notebook fulfills the task of building a comprehensive movie recommendation system using the MovieLens dataset. The objective is to develop and evaluate several models to suggest the Top-N movies for a given user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "inDo8q16HaSa",
        "outputId": "d88a2971-759b-4ed0-fa5f-80184684ca8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "592309eacf5041ebb2ccbbf1a23c2a9e",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zZbSAJTEo4H",
        "outputId": "63a97140-db6c-471d-d34b-b9358811f9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping scikit-surprise as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "ml-latest-small.zip 100%[===================>] 955.28K  1.08MB/s    in 0.9s    \n",
            "Archive:  ml-latest-small.zip\n",
            "   creating: ml-latest-small/\n",
            "  inflating: ml-latest-small/links.csv  \n",
            "  inflating: ml-latest-small/tags.csv  \n",
            "  inflating: ml-latest-small/ratings.csv  \n",
            "  inflating: ml-latest-small/README.txt  \n",
            "  inflating: ml-latest-small/movies.csv  \n"
          ]
        }
      ],
      "source": [
        "# Install surprise for our baseline SVD model\n",
        "\n",
        "!pip uninstall -y scikit-surprise\n",
        "!pip install scikit-surprise -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# Download and unzip the dataset\n",
        "!wget -q --show-progress http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip -o ml-latest-small.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR9Ihq_FFYY0"
      },
      "outputs": [],
      "source": [
        "# Define the path to the data\n",
        "PATH = Path(\"/content/ml-latest-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrI-XnkHwyw"
      },
      "source": [
        "## 2. Data Loading and Splitting\n",
        "\n",
        "We load the `ratings.csv` data and split it into a training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxMfWJPnH3Kw",
        "outputId": "6079fb4f-e3c1-4495-a4f7-f2649cc5d975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 80764\n",
            "Validation set size: 20072\n"
          ]
        }
      ],
      "source": [
        "# Load the full ratings dataset\n",
        "data = pd.read_csv(PATH/\"ratings.csv\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "np.random.seed(42)\n",
        "msk = np.random.rand(len(data)) < 0.8\n",
        "train_df = data[msk].copy()\n",
        "val_df = data[~msk].copy()\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cA6m6_vH_qP"
      },
      "source": [
        "## 3. Model 1: Baseline with Singular Value Decomposition (SVD)\n",
        "\n",
        "To fulfill the requirement of exploring SVD, we start with a robust implementation from the `scikit-surprise` library. This model is a form of matrix factorization optimized for handling sparse rating data, often referred to as FunkSVD. It will serve as our baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PdvYm-FIFGP",
        "outputId": "896d1e59-5ec6-4e45-d779-72c8facc2ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SVD++ hyperparameter tuning with GridSearchCV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed: 82.5min\n",
            "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 108.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GridSearchCV for SVD++ finished.\n",
            "Best cross-validation RMSE score: 0.8768\n",
            "Best parameters: {'n_factors': 80, 'n_epochs': 20, 'lr_all': 0.007, 'reg_all': 0.04, 'cache_ratings': True}\n",
            "\n",
            "Training the best SVD++ model on the full training set...\n",
            "\n",
            "Validation RMSE of the final SVD++ model:\n",
            "RMSE: 0.8626\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8625526193406733"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "from surprise import SVDpp # Import SVDpp instead of SVD\n",
        "\n",
        "# The Reader class is used to parse a file containing ratings.\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "\n",
        "# Load the training data into surprise's data format\n",
        "train_data = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# --- Hyperparameter Tuning for SVD++ ---\n",
        "# This part is correct and does not need to change.\n",
        "param_grid = {\n",
        "    'n_factors': [50, 80],\n",
        "    'n_epochs': [20, 30],\n",
        "    'lr_all': [0.005, 0.007],\n",
        "    'reg_all': [0.02, 0.04],\n",
        "    'cache_ratings': [True]\n",
        "}\n",
        "\n",
        "print(\"Starting SVD++ hyperparameter tuning with GridSearchCV...\")\n",
        "gs = GridSearchCV(SVDpp, param_grid, measures=['rmse'], cv=3, joblib_verbose=2)\n",
        "gs.fit(train_data)\n",
        "\n",
        "# --- Get the best SVD++ model and TRAIN IT ---\n",
        "print(\"\\nGridSearchCV for SVD++ finished.\")\n",
        "print(f\"Best cross-validation RMSE score: {gs.best_score['rmse']:.4f}\")\n",
        "print(\"Best parameters:\", gs.best_params['rmse'])\n",
        "\n",
        "# The best estimator is an UNTRAINED algorithm with the best parameters\n",
        "svd_model_blueprint = gs.best_estimator['rmse']\n",
        "\n",
        "# Build the full trainset from ALL of our training data\n",
        "full_trainset = train_data.build_full_trainset()\n",
        "\n",
        "# *** THIS IS THE CRUCIAL FIX ***\n",
        "# Train the best model on the full training set\n",
        "print(\"\\nTraining the best SVD++ model on the full training set...\")\n",
        "svd_model_blueprint.fit(full_trainset)\n",
        "\n",
        "# Now, the model is fully trained and we can use it for predictions\n",
        "svd_model = svd_model_blueprint\n",
        "\n",
        "# --- Final Evaluation on the held-out validation set ---\n",
        "val_data = Dataset.load_from_df(val_df[['userId', 'movieId', 'rating']], reader)\n",
        "valset = val_data.build_full_trainset().build_testset()\n",
        "\n",
        "predictions = svd_model.test(valset)\n",
        "print(\"\\nValidation RMSE of the final SVD++ model:\")\n",
        "accuracy.rmse(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmgsLWZkIasW"
      },
      "source": [
        "## 4. Data Encoding for PyTorch Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY65rqh8IbfO",
        "outputId": "9afb0a4a-01d9-4c1d-b397-62b591a0a5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique users: 610\n",
            "Number of unique movies: 8985\n",
            "\n",
            "Encoded Training Data Head:\n",
            "   userId  movieId  rating  timestamp\n",
            "0       0        0     4.0  964982703\n",
            "2       0        1     4.0  964982224\n",
            "3       0        2     5.0  964983815\n",
            "4       0        3     5.0  964982931\n",
            "5       0        4     3.0  964982400\n"
          ]
        }
      ],
      "source": [
        "def encode_data(df, train_ref=None):\n",
        "    \"\"\"Encodes user and movie ids into continuous integers.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    if train_ref is not None:\n",
        "        user_map = {o: i for i, o in enumerate(train_ref['userId'].unique())}\n",
        "        movie_map = {o: i for i, o in enumerate(train_ref['movieId'].unique())}\n",
        "    else:\n",
        "        user_map = {o: i for i, o in enumerate(df['userId'].unique())}\n",
        "        movie_map = {o: i for i, o in enumerate(df['movieId'].unique())}\n",
        "\n",
        "    df[\"userId\"] = df[\"userId\"].map(user_map).fillna(-1).astype(int)\n",
        "    df[\"movieId\"] = df[\"movieId\"].map(movie_map).fillna(-1).astype(int)\n",
        "\n",
        "    df = df[(df[\"userId\"] >= 0) & (df[\"movieId\"] >= 0)]\n",
        "    return df, user_map, movie_map\n",
        "\n",
        "# Encode the datasets and get the mappings\n",
        "df_train_encoded, user_map, movie_map = encode_data(train_df)\n",
        "df_val_encoded, _, _ = encode_data(val_df, train_ref=train_df)\n",
        "\n",
        "# Invert mappings for later use (to get original IDs back)\n",
        "user_inv_map = {i: o for o, i in user_map.items()}\n",
        "movie_inv_map = {i: o for o, i in movie_map.items()}\n",
        "\n",
        "num_users = len(user_map)\n",
        "num_items = len(movie_map)\n",
        "\n",
        "print(f\"Number of unique users: {num_users}\")\n",
        "print(f\"Number of unique movies: {num_items}\")\n",
        "print(\"\\nEncoded Training Data Head:\")\n",
        "print(df_train_encoded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLzEaGvxIjpl"
      },
      "source": [
        "## 5. Models: MF, MF_bias, and Neural Network\n",
        "\n",
        "Now, we build our custom models in PyTorch. This includes two matrix factorization models (one plain, one with biases) and a neural network model for the optional enhancement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaWEj2AaIrKU"
      },
      "outputs": [],
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, emb_size=100):\n",
        "        super(MF, self).__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
        "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
        "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
        "    def forward(self, u, v):\n",
        "        return (self.user_emb(u) * self.item_emb(v)).sum(1)\n",
        "\n",
        "class MF_bias(nn.Module):\n",
        "    def __init__(self, num_users, num_items, emb_size=100):\n",
        "        super(MF_bias, self).__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
        "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
        "        self.user_bias.weight.data.uniform_(-0.01, 0.01)\n",
        "        self.item_bias.weight.data.uniform_(-0.01, 0.01)\n",
        "    def forward(self, u, v):\n",
        "        dot = (self.user_emb(u) * self.item_emb(v)).sum(1)\n",
        "        u_bias = self.user_bias(u).squeeze()\n",
        "        v_bias = self.item_bias(v).squeeze()\n",
        "        return dot + u_bias + v_bias\n",
        "\n",
        "class CollabFNet(nn.Module):\n",
        "    def __init__(self, num_users, num_items, emb_size=100, n_hidden=10):\n",
        "        super(CollabFNet, self).__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
        "        self.lin1 = nn.Linear(emb_size * 2, n_hidden)\n",
        "        self.lin2 = nn.Linear(n_hidden, 1)\n",
        "        self.drop1 = nn.Dropout(0.1)\n",
        "    def forward(self, u, v):\n",
        "        x = F.relu(torch.cat([self.user_emb(u), self.item_emb(v)], dim=1))\n",
        "        x = self.drop1(x)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "        return x.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4guZjMqTIyzU"
      },
      "source": [
        "## 6. Training the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il8j5T5wI0l0",
        "outputId": "d33f766d-3eb1-4a67-e19f-9590abf90546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training MF Model ---\n",
            "Epoch 2/40 - Train Loss: 11.9766, Val Loss: 10.6572\n",
            "Epoch 4/40 - Train Loss: 8.9021, Val Loss: 7.0130\n",
            "Epoch 6/40 - Train Loss: 4.9849, Val Loss: 3.1872\n",
            "Epoch 8/40 - Train Loss: 1.7434, Val Loss: 1.0692\n",
            "Epoch 10/40 - Train Loss: 1.0928, Val Loss: 1.8434\n",
            "Epoch 12/40 - Train Loss: 2.3794, Val Loss: 2.7812\n",
            "\n",
            "Early stopping triggered after 13 epochs.\n",
            "\n",
            "Finished training. Loading best model with Val Loss: 1.0692\n",
            "\n",
            "--- Training MF_bias Model (Tuned) ---\n",
            "Epoch 2/40 - Train Loss: 9.1600, Val Loss: 4.4257\n",
            "Epoch 4/40 - Train Loss: 1.1516, Val Loss: 2.5739\n",
            "Epoch 6/40 - Train Loss: 3.7498, Val Loss: 2.6419\n",
            "Epoch 8/40 - Train Loss: 1.0760, Val Loss: 0.9966\n",
            "Epoch 10/40 - Train Loss: 0.7808, Val Loss: 1.1167\n",
            "Epoch 12/40 - Train Loss: 1.1169, Val Loss: 1.4308\n",
            "Epoch 14/40 - Train Loss: 1.3039, Val Loss: 1.4355\n",
            "\n",
            "Early stopping triggered after 14 epochs.\n",
            "\n",
            "Finished training. Loading best model with Val Loss: 0.9762\n",
            "\n",
            "--- Training Neural Network Model ---\n",
            "Epoch 2/40 - Train Loss: 10.0557, Val Loss: 8.2509\n",
            "Epoch 4/40 - Train Loss: 6.3796, Val Loss: 4.7064\n",
            "Epoch 6/40 - Train Loss: 3.2184, Val Loss: 2.1038\n",
            "Epoch 8/40 - Train Loss: 1.4120, Val Loss: 1.2149\n",
            "Epoch 10/40 - Train Loss: 1.4935, Val Loss: 1.9791\n",
            "Epoch 12/40 - Train Loss: 2.4244, Val Loss: 2.5677\n",
            "\n",
            "Early stopping triggered after 13 epochs.\n",
            "\n",
            "Finished training. Loading best model with Val Loss: 1.2149\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "def train_pytorch_model(model, epochs=40, lr=0.01, wd=1e-4):\n",
        "    \"\"\"\n",
        "    Improved training loop with Early Stopping and Learning Rate Scheduling.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    # Reduces learning rate when validation loss has stopped improving\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "    users_train = torch.LongTensor(df_train_encoded.userId.values)\n",
        "    items_train = torch.LongTensor(df_train_encoded.movieId.values)\n",
        "    ratings_train = torch.FloatTensor(df_train_encoded.rating.values)\n",
        "\n",
        "    users_val = torch.LongTensor(df_val_encoded.userId.values)\n",
        "    items_val = torch.LongTensor(df_val_encoded.movieId.values)\n",
        "    ratings_val = torch.FloatTensor(df_val_encoded.rating.values)\n",
        "\n",
        "    # --- Early Stopping variables ---\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_no_improve = 0\n",
        "    patience = 5  # Number of epochs to wait for improvement before stopping\n",
        "\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        y_hat = model(users_train, items_train)\n",
        "        loss = F.mse_loss(y_hat, ratings_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_hat_val = model(users_val, items_val)\n",
        "            val_loss = F.mse_loss(y_hat_val, ratings_val)\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if (i+1) % 2 == 0:\n",
        "            print(f\"Epoch {i+1}/{epochs} - Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        # --- Early Stopping logic ---\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            # Use deepcopy to ensure the best model state is not a reference\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "            print(f\"\\nEarly stopping triggered after {i+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    # Load the best model state before returning\n",
        "    if best_model_state:\n",
        "        print(f\"\\nFinished training. Loading best model with Val Loss: {best_val_loss:.4f}\")\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Re-train ALL models with the final, robust training function ---\n",
        "\n",
        "# Train the MF model\n",
        "print(\"--- Training MF Model ---\")\n",
        "mf_model = MF(num_users, num_items, emb_size=100)\n",
        "mf_model = train_pytorch_model(mf_model, epochs=40, lr=0.02, wd=1e-5)\n",
        "\n",
        "# Train the MF_bias model with the tuned aggressive learning rate\n",
        "print(\"\\n--- Training MF_bias Model (Tuned) ---\")\n",
        "mf_bias_model = MF_bias(num_users, num_items, emb_size=100)\n",
        "mf_bias_model = train_pytorch_model(mf_bias_model, epochs=40, lr=0.05, wd=1e-5)\n",
        "\n",
        "# Train the CollabFNet model\n",
        "print(\"\\n--- Training Neural Network Model ---\")\n",
        "collab_net_model = CollabFNet(num_users, num_items, emb_size=100)\n",
        "collab_net_model = train_pytorch_model(collab_net_model, epochs=40, lr=0.01, wd=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcZR7q2AJA10"
      },
      "source": [
        "## 7. Evaluation Metrics: Precision@K, Recall@K, NDCG@K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P3sNwAjJBnW",
        "outputId": "059aa56b-cabf-4348-c6b7-6f93f89c6319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Baseline SVD Model ---\n",
            "{'Precision@K': 0.5790540540540541, 'Recall@K': 0.6816506551842707, 'NDCG@K': 0.8070618866342171}\n",
            "\n",
            "--- Evaluating PyTorch MF Model ---\n",
            "{'Precision@K': 0.5272419627749577, 'Recall@K': 0.6631758979579774, 'NDCG@K': 0.7337122147650847}\n",
            "\n",
            "--- Evaluating PyTorch MF_bias Model ---\n",
            "{'Precision@K': 0.5710659898477157, 'Recall@K': 0.6872413274344841, 'NDCG@K': 0.7989320940005483}\n",
            "\n",
            "--- Evaluating PyTorch Neural Network Model ---\n",
            "{'Precision@K': 0.4798646362098139, 'Recall@K': 0.6263591190379837, 'NDCG@K': 0.6613086067122192}\n"
          ]
        }
      ],
      "source": [
        "def get_top_n_for_eval(predictions, n=10):\n",
        "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\"\"\"\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = [iid for (iid, _) in user_ratings[:n]]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "def calculate_ranking_metrics(predictions, k=10, threshold=4.0):\n",
        "    \"\"\"Calculate Precision@K, Recall@K, and NDCG@K.\"\"\"\n",
        "\n",
        "    # Get the top-K recommendations for each user\n",
        "    top_k = get_top_n_for_eval(predictions, n=k)\n",
        "\n",
        "    # Get the actual relevant items for each user\n",
        "    actuals = defaultdict(list)\n",
        "    for uid, iid, r, _, _ in predictions:\n",
        "        if r >= threshold:\n",
        "            actuals[uid].append(iid)\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    ndcgs = dict()\n",
        "\n",
        "    for uid, recs in top_k.items():\n",
        "        if uid not in actuals:\n",
        "            continue # User has no relevant items in validation set\n",
        "\n",
        "        # Metrics calculation\n",
        "        hits = len(set(recs) & set(actuals[uid]))\n",
        "        precisions[uid] = hits / k\n",
        "        recalls[uid] = hits / len(actuals[uid]) if actuals[uid] else 0\n",
        "\n",
        "        # NDCG calculation\n",
        "        relevance_scores = [1 if item in actuals[uid] else 0 for item in recs]\n",
        "        dcg = sum([rel / math.log2(i + 2) for i, rel in enumerate(relevance_scores)])\n",
        "        idcg = sum([1 / math.log2(i + 2) for i in range(min(len(actuals[uid]), k))])\n",
        "        ndcgs[uid] = dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    avg_precision = sum(prec for prec in precisions.values()) / len(precisions) if precisions else 0\n",
        "    avg_recall = sum(rec for rec in recalls.values()) / len(recalls) if recalls else 0\n",
        "    avg_ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs) if ndcgs else 0\n",
        "\n",
        "    return {\"Precision@K\": avg_precision, \"Recall@K\": avg_recall, \"NDCG@K\": avg_ndcg}\n",
        "\n",
        "# Evaluate the baseline SVD model\n",
        "print(\"--- Evaluating Baseline SVD Model ---\")\n",
        "svd_predictions = svd_model.test(valset)\n",
        "svd_metrics = calculate_ranking_metrics(svd_predictions)\n",
        "print(svd_metrics)\n",
        "\n",
        "# --- Helper function to evaluate PyTorch models ---\n",
        "def evaluate_pytorch_model(model, val_df, k=10, threshold=4.0):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for _, row in val_df.iterrows():\n",
        "            original_uid = row['userId']\n",
        "            original_iid = row['movieId']\n",
        "            true_rating = row['rating']\n",
        "\n",
        "            # Predict only for items/users seen in training\n",
        "            if original_uid in user_map and original_iid in movie_map:\n",
        "                encoded_uid = user_map[original_uid]\n",
        "                encoded_iid = movie_map[original_iid]\n",
        "\n",
        "                user_tensor = torch.LongTensor([encoded_uid])\n",
        "                item_tensor = torch.LongTensor([encoded_iid])\n",
        "\n",
        "                est = model(user_tensor, item_tensor).item()\n",
        "                all_preds.append((original_uid, original_iid, true_rating, est, None))\n",
        "\n",
        "    return calculate_ranking_metrics(all_preds, k=k, threshold=threshold)\n",
        "\n",
        "# Evaluate the custom PyTorch models\n",
        "print(\"\\n--- Evaluating PyTorch MF Model ---\")\n",
        "mf_metrics = evaluate_pytorch_model(mf_model, val_df)\n",
        "print(mf_metrics)\n",
        "\n",
        "print(\"\\n--- Evaluating PyTorch MF_bias Model ---\")\n",
        "mf_bias_metrics = evaluate_pytorch_model(mf_bias_model, val_df)\n",
        "print(mf_bias_metrics)\n",
        "\n",
        "print(\"\\n--- Evaluating PyTorch Neural Network Model ---\")\n",
        "collab_net_metrics = evaluate_pytorch_model(collab_net_model, val_df)\n",
        "print(collab_net_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqHTFtUHJL2b"
      },
      "outputs": [],
      "source": [
        "def recommend_movies(user_id, N, model=svd_model):\n",
        "    \"\"\"\n",
        "    Recommends N movies for a given user using the best-performing model (SVD).\n",
        "\n",
        "    Args:\n",
        "        user_id (int): The original ID of the user.\n",
        "        N (int): The number of movies to recommend.\n",
        "        model: The trained surprise SVD model.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the top N recommended movie titles and genres.\n",
        "                      Returns a message if the user is unknown.\n",
        "    \"\"\"\n",
        "    # Check if the user ID exists in the training data\n",
        "    try:\n",
        "        # Surprise uses inner IDs, but we can check if the raw ID is known\n",
        "        model.trainset.to_inner_uid(user_id)\n",
        "    except ValueError:\n",
        "        return f\"User ID {user_id} not found in the training data.\"\n",
        "\n",
        "    # Get a list of all movie IDs from the training set\n",
        "    all_movie_ids = train_df['movieId'].unique()\n",
        "\n",
        "    # Get movies the user has already rated from the training set\n",
        "    rated_movie_ids = train_df[train_df['userId'] == user_id]['movieId'].unique()\n",
        "\n",
        "    # Predict ratings for movies the user has NOT rated\n",
        "    unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "    predictions = [model.predict(user_id, movie_id) for movie_id in unrated_movie_ids]\n",
        "\n",
        "    # Sort the predictions by the estimated rating\n",
        "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
        "\n",
        "    # Get the top N movie IDs from the sorted predictions\n",
        "    top_n_movie_ids = [pred.iid for pred in predictions[:N]]\n",
        "\n",
        "    # Get movie titles and genres from the original movies dataframe\n",
        "    movies_df = pd.read_csv(PATH/\"movies.csv\")\n",
        "    recommendations = movies_df[movies_df['movieId'].isin(top_n_movie_ids)]\n",
        "\n",
        "    # Reorder the dataframe to match the recommendation order\n",
        "    recommendations = recommendations.set_index('movieId').loc[top_n_movie_ids].reset_index()\n",
        "\n",
        "    return recommendations[['title', 'genres']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrGyURg3JOSC",
        "outputId": "7ce8a078-fea6-4138-fa6a-1c79240ca5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 recommendations for User ID 100:\n",
            "\n",
            "                                                                                                             title                                genres\n",
            "                                                                                         Lawrence of Arabia (1962)                   Adventure|Drama|War\n",
            "Neon Genesis Evangelion: The End of Evangelion (Shin seiki Evangelion Gekijô-ban: Air/Magokoro wo, kimi ni) (1997) Action|Animation|Drama|Fantasy|Sci-Fi\n",
            "                                                                   Seventh Seal, The (Sjunde inseglet, Det) (1957)                                 Drama\n",
            "                                                                                  Streetcar Named Desire, A (1951)                                 Drama\n",
            "                                                                                                Rear Window (1954)                      Mystery|Thriller\n",
            "                                                                                             Cool Hand Luke (1967)                                 Drama\n",
            "                                                                                                  Gallipoli (1981)                             Drama|War\n",
            "                                                                             Fear and Loathing in Las Vegas (1998)                Adventure|Comedy|Drama\n",
            "                                                                                    Philadelphia Story, The (1940)                  Comedy|Drama|Romance\n",
            "                                                                                  Shawshank Redemption, The (1994)                           Crime|Drama\n"
          ]
        }
      ],
      "source": [
        "# --- Example Usage ---\n",
        "# Get 10 recommendations for user with original ID 100\n",
        "user_to_recommend = 100\n",
        "num_recommendations = 10\n",
        "\n",
        "recommended_list = recommend_movies(user_to_recommend, num_recommendations)\n",
        "print(f\"Top {num_recommendations} recommendations for User ID {user_to_recommend}:\\n\")\n",
        "print(recommended_list.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZvJOEHP_gBE",
        "outputId": "a833df4e-a615-4142-a1c6-db02339f1d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All necessary files have been saved to the 'hf_space_streamlit_deploy' directory.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# Create a new directory for the Streamlit deployment files\n",
        "os.makedirs(\"hf_space_streamlit_deploy\", exist_ok=True)\n",
        "\n",
        "# 1. Save the best model (your trained SVD++ model)\n",
        "with open('hf_space_streamlit_deploy/svd_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svd_model, f)\n",
        "\n",
        "# 2. Save the training data. The app needs this to know which movies a user has already seen.\n",
        "train_df.to_csv('hf_space_streamlit_deploy/train_data.csv', index=False)\n",
        "\n",
        "# 3. Copy the movies.csv file for getting movie titles\n",
        "!cp ml-latest-small/movies.csv hf_space_streamlit_deploy/movies.csv\n",
        "\n",
        "print(\"All necessary files have been saved to the 'hf_space_streamlit_deploy' directory.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}